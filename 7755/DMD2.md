<h1 id="dmd2方法的核心優勢">DMD2方法的核心優勢</h1>
<h2 id="白話懶人包：dmd2是一種可以讓算圖所需的步數變少的加速器。">白話懶人包：DMD2是一種可以讓算圖所需的步數變少的加速器。</h2>
<p>-# 缺點是：會犧牲掉成果的多樣性</p>
<p>以下是目前PixAI多數人用的DMD2 LoRA：</p>
<ul>
<li><a href="https://pixai.art/zh/model/1791670597789823714-dmd2sdxl4steplora">dmd2_sdxl_4step_lora</a></li>
<li><a href="https://pixai.art/zh/model/1810009785472319745-DMD2">DMD2</a><br>
上述這兩個其實是一樣的，參數：</li>
</ul>
<pre><code>LoRA Weight：1
step: 8
CFG: 1
</code></pre>
<pre><code>LoRA Weight：0.7
step: 8
CFG: 1.6
</code></pre>
<ul>
<li><a href="https://pixai.art/zh/model/1897811443252033347-DMD2HyperSDXL">DMD2+HyperSDXL</a><br>
參數：</li>
</ul>
<pre><code>LoRA Weight：1
步數：4
CFG: 1
</code></pre>
<hr>
<h3 id="原始論文：httpsarxiv.orgabs2405.14867">原始論文：<a href="https://arxiv.org/abs/2405.14867">https://arxiv.org/abs/2405.14867</a></h3>
<p>**DMD2 (Improved Distribution Matching Distillation)**是一種改進的擴散模型蒸餾技術，能將原本需要多步驟的擴散模型加速成只需1-4步的快速生成器。</p>
<h2 id="以下為詳細原理說明，有興趣再看，請搭配原始論文">以下為詳細原理說明，有興趣再看，請搭配原始論文</h2>
<h3 id="主要貢獻與改進">主要貢獻與改進</h3>
<ol>
<li>
<p>移除回歸損失 (第4.1節)<br>
原始DMD方法需要預先生成大量噪聲-圖像配對資料，這在大規模文本到圖像合成任務中成本極高。DMD2成功移除了這個回歸損失需求，大幅降低了訓練成本。</p>
</li>
<li>
<p>兩時間尺度更新規則 (第4.2節、附錄C)<br>
論文發現移除回歸損失後會導致訓練不穩定，原因是假分數函數(fake score function)無法準確追蹤生成器的輸出分布。解決方案是採用兩時間尺度更新規則：每更新一次生成器，就更新5次假擴散模型。</p>
</li>
<li>
<p>整合GAN損失 (第4.3節)<br>
加入GAN判別器來區分真實圖像和生成圖像，這讓學生模型能在真實數據上訓練，從而緩解教師模型的近似誤差，提升圖像品質。</p>
</li>
<li>
<p>多步驟生成器與反向模擬 (第4.4-4.5節)</p>
</li>
</ol>
<ul>
<li>支援多步驟採樣（如4步生成）</li>
<li>提出「反向模擬」技術：在訓練時模擬推理時的生成器輸入，避免訓練與推理階段的輸入不匹配問題</li>
</ul>
<h3 id="實驗結果-第5節">實驗結果 (第5節)</h3>
<p><strong>ImageNet-64×64 (表1)</strong></p>
<ul>
<li>單步生成達到FID 1.28，顯著優於原始DMD的2.62</li>
<li>甚至超越了需要511步的教師模型(ODE採樣器FID 2.32)</li>
</ul>
<p><strong>COCO 2014文本到圖像 (表2、表5)</strong></p>
<ul>
<li>SDXL蒸餾：4步生成FID 19.32，超越教師模型(FID 19.36)</li>
<li>SD v1.5蒸餾：單步生成FID 8.35，大幅領先原始DMD的11.49</li>
</ul>
<p><strong>人類評估 (圖5)</strong><br>
在圖像品質和文本對齊度上都優於其他蒸餾方法，甚至在24%的樣本上超越教師模型的圖像品質。</p>
<h3 id="技術細節與消融研究-表3-4">技術細節與消融研究 (表3-4)</h3>
<p>消融研究顯示各組件的重要性：</p>
<ul>
<li>僅移除回歸損失會導致FID惡化至3.48</li>
<li>加入兩時間尺度更新規則後恢復到2.61</li>
<li>再加入GAN損失後達到最佳的1.51</li>
</ul>
<h3 id="局限性-第6節">局限性 (第6節)</h3>
<ul>
<li>圖像多樣性略有下降</li>
<li>SDXL模型仍需4步才能達到最佳品質</li>
<li>訓練時使用固定的引導尺度，限制了用戶靈活性</li>
</ul>
<p>這項研究的突破在於成功實現了真正的分布匹配蒸餾，不僅大幅降低了訓練成本，還讓蒸餾後的模型能夠超越原始教師模型的性能，這在之前的方法中是難以實現的。</p>
<h3 id="大幅減少採樣步數-第1節、第5節">大幅減少採樣步數 (第1節、第5節)</h3>
<p><strong>從多步到單步</strong></p>
<ul>
<li><strong>原始教師模型</strong>：通常需要50-100步甚至更多步驟</li>
<li><strong>DMD2蒸餾後</strong>：僅需1-4步</li>
</ul>
<p><strong>具體案例比較 (表2)</strong><br>
以SDXL模型為例：</p>
<ul>
<li><strong>原始SDXL</strong>：100步採樣，FID 19.36</li>
<li><strong>DMD2 4步版本</strong>：4步採樣，FID 19.32</li>
<li><strong>DMD2 1步版本</strong>：1步採樣，FID 19.01</li>
</ul>
<p>這代表推理速度提升了<strong>25-100倍</strong>！</p>
<h3 id="速度提升的實際影響-第1節">速度提升的實際影響 (第1節)</h3>
<p>論文提到原始擴散模型的迭代去噪過程使得高解析度文本到圖像合成變得「緩慢且昂貴」(slow and expensive)。DMD2通過將採樣步數從數十步減少到1-4步，實現了約<strong>500倍的推理成本降低</strong>。</p>
<h3 id="性能對比-表1、表5">性能對比 (表1、表5)</h3>
<p>即使大幅減少步數，圖像品質反而提升了：</p>
<ul>
<li>ImageNet上單步生成超越511步的教師模型</li>
<li>COCO上4步生成與100步教師模型品質相當</li>
</ul>
<p>這種加速方法就像是把原本需要慢慢「擴散」100步的過程，壓縮成只要「一步到位」或「四步搞定」</p>

